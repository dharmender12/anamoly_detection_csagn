{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee29f172",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os,glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a720d4e7",
   "metadata": {},
   "source": [
    "# CSV files to NPY files\n",
    "\n",
    "## Step 1 :  change all csv files to npy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6db654df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the conversion is complete\n"
     ]
    }
   ],
   "source": [
    "path =glob.glob('lcs/*.csv')\n",
    "#print(len(path))\n",
    "dummy_i=1\n",
    "c=[]\n",
    "for x in path:\n",
    "    data=np.genfromtxt(x,delimiter=',')\n",
    "        \n",
    "        #listing =df.convert_dtype('float64')\n",
    "        #print(listing)\n",
    "        #c.append(data)\n",
    "    np.save('npy/'+str(dummy_i)+'.npy',data)\n",
    "    dummy_i=dummy_i+1\n",
    "    \n",
    "print('the conversion is complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28a8ea1",
   "metadata": {},
   "source": [
    "Check whether the files are equal or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75a439a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46411\n"
     ]
    }
   ],
   "source": [
    "path =glob.glob('npy/*.npy')\n",
    "print(len(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fa2868",
   "metadata": {},
   "source": [
    "# Change the npy files to a single npy file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20684766",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20114/2927616354.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  all_array =np.array(all_arrays)\n"
     ]
    }
   ],
   "source": [
    "path =glob.glob('npy/*.npy')\n",
    "all_arrays =[]\n",
    "path.sort()\n",
    "for npfiles in path:\n",
    "    all_arrays.append(np.load(npfiles).astype(np.float64))    \n",
    "all_array =np.array(all_arrays)\n",
    "np.save('lcs.npy',all_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07924523",
   "metadata": {},
   "source": [
    "Check the file by loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f673193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46411\n",
      "[[5.82033773e+04 1.84048805e+01 3.86154652e-02]\n",
      " [5.82063446e+04 1.83142738e+01 3.62164453e-02]\n",
      " [5.82063656e+04 1.83537350e+01 3.72382440e-02]\n",
      " [5.82103880e+04 1.82501984e+01 3.46301571e-02]\n",
      " [5.82293366e+04 1.83590126e+01 3.73775586e-02]]\n"
     ]
    }
   ],
   "source": [
    "lcs =np.load('lcs.npy',allow_pickle=True)\n",
    "print(len(lcs))\n",
    "print(lcs[100][0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47603900",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#path =glob.glob('lcs/*.csv')\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#def data():\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#data =data()\u001b[39;00m\n\u001b[1;32m     10\u001b[0m lcs \u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlcs.npy\u001b[39m\u001b[38;5;124m'\u001b[39m,allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequence\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pad_sequences\n\u001b[1;32m     13\u001b[0m lcs_raw \u001b[38;5;241m=\u001b[39m pad_sequences(lcs, value\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mnan, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat\u001b[39m\u001b[38;5;124m'\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#functions taken from https://github.com/yutarotachibana/CatalinaQSO_AutoEncoder\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "#path =glob.glob('lcs/*.csv')\n",
    "\n",
    "#def data():\n",
    "#    for x in path:\n",
    "#        df =pd.read_csv(x,usecols=(0,1,2))\n",
    "    #print(df)\n",
    "    \n",
    "#data =data()\n",
    "\n",
    "lcs =np.load('lcs.npy',allow_pickle=True)\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "lcs_raw = pad_sequences(lcs, value=np.nan, dtype='float', padding='post')\n",
    "#functions taken from https://github.com/yutarotachibana/CatalinaQSO_AutoEncoder\n",
    "\n",
    "def times_to_lags(T):\n",
    "    \"\"\"(N x n_step) matrix of times -> (N x n_step) matrix of lags.\n",
    "    First time is assumed to be zero.\n",
    "    \"\"\"\n",
    "    assert T.ndim == 2, \"T must be an (N x n_step) matrix\"\n",
    "    return np.c_[np.diff(T, axis=1)/365., np.zeros(T.shape[0])]\n",
    "\n",
    "def preprocess(X_raw, m_max=np.inf):\n",
    "    X = X_raw.copy()\n",
    "    #print(X)\n",
    "    wrong_units =  np.all(np.isnan(X[:, :, 1])) | (np.nanmax(X[:, :, 1], axis=1) > m_max)\n",
    "    #print(wrong_units)\n",
    "    X = X[~wrong_units, :, :]\n",
    "    X[:, :, 0] = times_to_lags(X[:, :, 0])\n",
    "    #print(X[:, :, 0])\n",
    "    means = np.atleast_2d(np.nanmean(X[:, :, 1], axis=1)).T\n",
    "    #print(means)\n",
    "    X[:, :, 1] -= means\n",
    "    scales = np.atleast_2d(np.nanstd(X[:, :, 1], axis=1)).T\n",
    "    #print(scales)\n",
    "    X[:, :, 1] /= scales\n",
    "    errors = X[:, :, 2] / scales\n",
    "    #print(errors)\n",
    "    X = X[:, :, :2]\n",
    "    #print(X) is lcs_scaled value\n",
    "    return X, means, scales, errors, wrong_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab0f090",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(lcs_raw)\n",
    "lcs_scaled, means,scales, errors, wrong_units =preprocess(lcs_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc0011a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lcs_scaled[100][0:5])\n",
    "print(means[100])\n",
    "#print(lcs_scaled[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40e6c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path =glob.glob('lcs/*.csv')\n",
    "print(len(path))\n",
    "val =20\n",
    "count=0\n",
    "dummy_i=0\n",
    "#def check(mag, val):\n",
    " #   return (all(x>= val for x in mag))\n",
    "#print(check())\n",
    "for i in path:\n",
    "    df =pd.read_csv(i)\n",
    "    df.columns =['mjd','mag','err']\n",
    "    #if np.median(df['mag'])>=20:\n",
    "    if len(df['mjd'])<=100:   \n",
    "        new_file='less/'\n",
    "        rename_file_cmd ='mv' + '  ' + path[count] + '  ' + new_file\n",
    "        #rename_file_cmd ='mv' + '  ' + path2+  '  ' +path2+ new_file +'.csv'\n",
    "        os.system(rename_file_cmd)\n",
    "        count+=1\n",
    "        #print('the data points are greater than 100')\n",
    "    #dummy_i=dummy_i+1\n",
    "    #print(df.head())if (check(df['mag'],val)):\n",
    "    elif np.median(df['mag'])>=20:\n",
    "      #  if j >=val:\n",
    "        #print('magnitude is greater than or equal to 20'.format(count+1))\n",
    "        #print(i)\n",
    "        newer_file='median_above_20/'\n",
    "        rename_file_cmd ='mv' + '  ' + path[count] + '  ' + newer_file\n",
    "        os.system(rename_file_cmd)\n",
    "        count+=1\n",
    "    else: \n",
    "                newer_file='other/'\n",
    "                rename_file_cmd ='mv' + '  ' + path[count] + '  ' + newer_file\n",
    "                os.system(rename_file_cmd)\n",
    "        \n",
    "                count+=1\n",
    "           # break\n",
    "    #\n",
    "     #   print(i)\n",
    "        \n",
    "        \n",
    "     \n",
    "print(count)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682aa0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=glob.glob()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8351467c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path1=glob.glob('lcs/*.csv')\n",
    "print(len(path1))\n",
    "count=0\n",
    "for i in path1:\n",
    "    df = pd.read_csv(i)\n",
    "    df.columns = ['mjd','mag','err']\n",
    "    \n",
    "    if df['mag']<20:\n",
    "        count+=1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb6ae356",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'PyOrigin'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25860/3085117429.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mPyOrigin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'PyOrigin'"
     ]
    }
   ],
   "source": [
    "import PyOrigin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bde05a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad690946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob,os\n",
    "from keras.layers import Dense,GRU, Flatten, Conv2D, Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import (Callback, TensorBoard, EarlyStopping,\n",
    "                             ModelCheckpoint, CSVLogger, ProgbarLogger)\n",
    "from tensorflow.keras.layers import (Input, Dense, TimeDistributed, LSTM, GRU, Dropout, merge, \n",
    "                         Concatenate, Flatten, RepeatVector, Lambda, Bidirectional, SimpleRNN)\n",
    "import sys\n",
    "import csv\n",
    "from collections import Iterable, OrderedDict\n",
    "import datetime\n",
    "from sklearn.manifold import TSNE\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Input, Flatten, Dense, Lambda, Reshape\n",
    "from tensorflow.keras.layers import SimpleRNN,Bidirectional,BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from tensorflow.keras.losses import KLDivergence,binary_crossentropy\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.activations import relu, sigmoid\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234e9c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffling the data to avoid systematics \n",
    "np.random.seed(42)\n",
    "np.random.shuffle(lcs)\n",
    "lcs[100][0:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
